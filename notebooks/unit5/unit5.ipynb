{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/notebooks/unit5/unit5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3NL_e4crQv"
      },
      "source": [
        "# Unit 5: An Introduction to ML-Agents\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ZiytXEgqIz"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/thumbnail.png\" alt=\"Thumbnail\"/>\n",
        "\n",
        "In this notebook, you'll learn about ML-Agents and train two agents.\n",
        "\n",
        "- The first one will learn to **shoot snowballs onto spawning targets**.\n",
        "- The second need to press a button to spawn a pyramid, then navigate to the pyramid, knock it over, **and move to the gold brick at the top**. To do that, it will need to explore its environment, and we will use a technique called curiosity.\n",
        "\n",
        "After that, you'll be able **to watch your agents playing directly on your browser**.\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYrDriDujzX"
      },
      "source": [
        "‚¨áÔ∏è Here is an example of what **you will achieve at the end of this unit.** ‚¨áÔ∏è\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBmFlh8suma-"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids.gif\" alt=\"Pyramids\"/>\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget.gif\" alt=\"SnowballTarget\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-cYE0K5iL-w"
      },
      "source": [
        "### üéÆ Environments:\n",
        "\n",
        "- [Pyramids](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#pyramids)\n",
        "- SnowballTarget\n",
        "\n",
        "### üìö RL-Library:\n",
        "\n",
        "- [ML-Agents](https://github.com/Unity-Technologies/ml-agents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEhtaFh9i31S"
      },
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7f63r3Yi5vE"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "\n",
        "At the end of the notebook, you will:\n",
        "\n",
        "- Understand how works **ML-Agents**, the environment library.\n",
        "- Be able to **train agents in Unity Environments**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viNzVbVaYvY3"
      },
      "source": [
        "## This notebook is from the Deep Reinforcement Learning Course\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5HnEefISCB"
      },
      "source": [
        "In this free course, you will:\n",
        "\n",
        "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
        "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
        "- ü§ñ Train **agents in unique environments**\n",
        "\n",
        "And more check üìö the syllabus üëâ https://huggingface.co/deep-rl-course/communication/publishing-schedule\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "\n",
        "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mo_6rXIjRi"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö **Study [what is ML-Agents and how it works by reading Unit 5](https://huggingface.co/deep-rl-course/unit5/introduction)**  ü§ó  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYO1uD5Ujgdh"
      },
      "source": [
        "# Let's train our agents üöÄ\n",
        "\n",
        "**To validate this hands-on for the certification process, you just need to push your trained models to the Hub**. There‚Äôs no results to attain to validate this one. But if you want to get nice results you can try to attain:\n",
        "\n",
        "- For `Pyramids` : Mean Reward = 1.75\n",
        "- For `SnowballTarget` : Mean Reward = 15 or 30 targets hit in an episode.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DssdIjk_8vZE"
      },
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfCXHy68xBv"
      },
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clone the repository üîΩ\n",
        "\n",
        "- We need to clone the repository, that contains **ML-Agents.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Clone the repository (can take 3min)\n",
        "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup the Virtual Environment üîΩ\n",
        "- In order for the **ML-Agents** to run successfully in Colab,  Colab's Python version must meet the library's Python requirements.\n",
        "\n",
        "- We can check for the supported Python version under the `python_requires` parameter in the `setup.py` files. These files are required to set up the **ML-Agents** library for use and can be found in the following locations:\n",
        "  - `/content/ml-agents/ml-agents/setup.py`\n",
        "  - `/content/ml-agents/ml-agents-envs/setup.py`\n",
        "\n",
        "- Colab's Current Python version(can be checked using `!python --version`) doesn't match the library's `python_requires` parameter, as a result installation may silently fail and lead to errors like these, when executing the same commands later:\n",
        "  - `/bin/bash: line 1: mlagents-learn: command not found`\n",
        "  - `/bin/bash: line 1: mlagents-push-to-hf: command not found`\n",
        "\n",
        "- To resolve this, we'll create a virtual environment with a Python version compatible with the **ML-Agents** library.\n",
        "\n",
        "`Note:` *For future compatibility, always check the `python_requires` parameter in the installation files and set your virtual environment to the maximum supported Python version in the given below script if the Colab's Python version is not compatible*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "# Colab's Current Python Version (Incompatible with ML-Agents)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: virtualenv in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (20.29.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (from virtualenv) (0.3.9)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (from virtualenv) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (from virtualenv) (4.3.7)\n",
            "Requirement already satisfied: pip in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (25.0.1)\n",
            "Requirement already satisfied: ujson in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (5.10.0)\n"
          ]
        }
      ],
      "source": [
        "# Install virtualenv if not already installed\n",
        "!pip install virtualenv\n",
        "\n",
        "# Create and activate a virtual environment in the current directory\n",
        "!python -m venv venv\n",
        "!source venv/bin/activate\n",
        "\n",
        "# Install the specific Python version and dependencies\n",
        "!pip install --upgrade pip\n",
        "!pip install ujson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: virtualenv in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (20.29.3)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (from virtualenv) (0.3.9)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (from virtualenv) (3.18.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /home/stalaei/miniconda3/envs/DeepRL/lib/python3.10/site-packages (from virtualenv) (4.3.7)\n",
            "created virtual environment CPython3.10.12.final.0-64 in 1612ms\n",
            "  creator CPython3Posix(dest=/home/stalaei/projects/deep-rl-class/notebooks/unit5/myenv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/stalaei/.local/share/virtualenv)\n",
            "    added seed packages: pip==23.3.2, setuptools==69.0.3, wheel==0.42.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n",
            "--2025-03-28 12:09:51--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.191.158, 104.16.32.241, 2606:4700::6810:20f1, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.191.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154615621 (147M) [application/octet-stream]\n",
            "Saving to: ‚ÄòMiniconda3-latest-Linux-x86_64.sh‚Äô\n",
            "\n",
            "Miniconda3-latest-L 100%[===================>] 147.45M   228MB/s    in 0.6s    \n",
            "\n",
            "2025-03-28 12:09:51 (228 MB/s) - ‚ÄòMiniconda3-latest-Linux-x86_64.sh‚Äô saved [154615621/154615621]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "./Miniconda3-latest-Linux-x86_64.sh: 432: cannot create /usr/local/_conda: Permission denied\n",
            "/bin/bash: line 1: /usr/local/bin/activate: No such file or directory\n",
            "\n",
            "DirectoryNotACondaEnvironmentError: The target directory exists, but it is not a conda environment.\n",
            "Use 'conda create' to convert the directory to a conda environment.\n",
            "  target directory: /usr/local\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install virtualenv and create a virtual environment\n",
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "\n",
        "# Download and install Miniconda\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# Activate Miniconda and install Python ver 3.10.12\n",
        "!source /usr/local/bin/activate\n",
        "!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # Specify the version here\n",
        "\n",
        "# Set environment variables for Python and conda paths\n",
        "!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n",
        "!export CONDA_PREFIX=/usr/local/envs/myenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.16\n"
          ]
        }
      ],
      "source": [
        "# Python Version in New Virtual Environment (Compatible with ML-Agents)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installing the dependencies üîΩ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/stalaei/miniconda3/envs/DeepRL/bin/python\n"
          ]
        }
      ],
      "source": [
        "!which python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Go inside the repository and install the package (can take 3min)\n",
        "%cd ml-agents\n",
        "!pip3 install -e ./ml-agents-envs\n",
        "!pip3 install -e ./ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_7Ptd_kEcG"
      },
      "source": [
        "## SnowballTarget ‚õÑ\n",
        "\n",
        "If you need a refresher on how this environments work check this section üëâ\n",
        "https://huggingface.co/deep-rl-course/unit5/snowball-target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRY5ufKUKfhI"
      },
      "source": [
        "### Download and move the environment zip file in `./training-envs-executables/linux/`\n",
        "- Our environment executable is in a zip file.\n",
        "- We need to download it and place it to `./training-envs-executables/linux/`\n",
        "- We use a linux executable because we use colab, and colab machines OS is Ubuntu (linux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C9Ls6_6eOKiA"
      },
      "outputs": [],
      "source": [
        "# Here, we create training-envs-executables and linux\n",
        "!mkdir ./training-envs-executables\n",
        "!mkdir ./training-envs-executables/linux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekSh8LWawkB5"
      },
      "source": [
        "We downloaded the file SnowballTarget.zip from https://github.com/huggingface/Snowball-Target using `wget`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6LosWO50wa77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-28 12:13:15--  https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/huggingface/Snowball-Target/main/SnowballTarget.zip [following]\n",
            "--2025-03-28 12:13:16--  https://media.githubusercontent.com/media/huggingface/Snowball-Target/main/SnowballTarget.zip\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35134213 (34M) [application/zip]\n",
            "Saving to: ‚Äò./training-envs-executables/linux/SnowballTarget.zip‚Äô\n",
            "\n",
            "./training-envs-exe 100%[===================>]  33.51M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-03-28 12:13:17 (273 MB/s) - ‚Äò./training-envs-executables/linux/SnowballTarget.zip‚Äô saved [35134213/35134213]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\" -O ./training-envs-executables/linux/SnowballTarget.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LLVaEEK3ayi"
      },
      "source": [
        "We unzip the executable.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8FPx0an9IAwO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyumV5XfPKzu"
      },
      "source": [
        "Make sure your file is accessible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EdFsLJ11JvQf"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./training-envs-executables/linux/SnowballTarget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuEq32Mwvtz"
      },
      "source": [
        "### Define the SnowballTarget config file\n",
        "- In ML-Agents, you define the **training hyperparameters into config.yaml files.**\n",
        "\n",
        "There are multiple hyperparameters. To know them better, you should check for each explanation with [the documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)\n",
        "\n",
        "\n",
        "So you need to create a `SnowballTarget.yaml` config file in ./content/ml-agents/config/ppo/\n",
        "\n",
        "We'll give you here a first version of this config (to copy and paste into your `SnowballTarget.yaml file`), **but you should modify it**.\n",
        "\n",
        "```\n",
        "behaviors:\n",
        "  SnowballTarget:\n",
        "    trainer_type: ppo\n",
        "    summary_freq: 10000\n",
        "    keep_checkpoints: 10\n",
        "    checkpoint_interval: 50000\n",
        "    max_steps: 200000\n",
        "    time_horizon: 64\n",
        "    threaded: false\n",
        "    hyperparameters:\n",
        "      learning_rate: 0.0003\n",
        "      learning_rate_schedule: linear\n",
        "      batch_size: 128\n",
        "      buffer_size: 2048\n",
        "      beta: 0.005\n",
        "      epsilon: 0.2\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "    network_settings:\n",
        "      normalize: false\n",
        "      hidden_units: 256\n",
        "      num_layers: 2\n",
        "      vis_encode_type: simple\n",
        "    reward_signals:\n",
        "      extrinsic:\n",
        "        gamma: 0.99\n",
        "        strength: 1.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U3sRH4N4h_l"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config1.png\" alt=\"Config SnowballTarget\"/>\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config2.png\" alt=\"Config SnowballTarget\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJJdo_5AyoGo"
      },
      "source": [
        "As an experimentation, you should also try to modify some other hyperparameters. Unity provides very [good documentation explaining each of them here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
        "\n",
        "Now that you've created the config file and understand what most hyperparameters do, we're ready to train our agent üî•."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fI555bO12v"
      },
      "source": [
        "### Train the agent\n",
        "\n",
        "To train our agent, we just need to **launch mlagents-learn and select the executable containing the environment.**\n",
        "\n",
        "We define four parameters:\n",
        "\n",
        "1. `mlagents-learn <config>`: the path where the hyperparameter config file is.\n",
        "2. `--env`: where the environment executable is.\n",
        "3. `--run_id`: the name you want to give to your training run id.\n",
        "4. `--no-graphics`: to not launch the visualization during the training.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentslearn.png\" alt=\"MlAgents learn\"/>\n",
        "\n",
        "Train the model and use the `--resume` flag to continue training in case of interruption.\n",
        "\n",
        "> It will fail first time if and when you use `--resume`, try running the block again to bypass the error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN32oWF8zPjs"
      },
      "source": [
        "The training will take 10 to 35min depending on your config, go take a ‚òïÔ∏èyou deserve it ü§ó."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bS-Yh1UdHfzy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "            ‚îê  ‚ïñ\n",
            "        ‚ïì‚ïñ‚ï¨‚îÇ‚ï°  ‚îÇ‚îÇ‚ï¨‚ïñ‚ïñ\n",
            "    ‚ïì‚ïñ‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îò  ‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚ï¨‚ïñ\n",
            " ‚ïñ‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚ï¨‚ïú        ‚ïô‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚ïñ‚ïñ                               ‚ïó‚ïó‚ïó\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚ïñ‚îÇ‚îÇ‚ï¶‚ïñ        ‚ïñ‚ï¨‚îÇ‚îÇ‚ïó‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£             ‚ïú‚ïú‚ïú  ‚ïü‚ï£‚ï£\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ïñ‚îÇ‚ï¨‚ïñ‚ïñ‚ïì‚ï¨‚ï™‚îÇ‚ïì‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£ ‚ïí‚ï£‚ï£‚ïñ‚ïó‚ï£‚ï£‚ï£‚ïó   ‚ï£‚ï£‚ï£ ‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£ ‚ïü‚ï£‚ï£‚ïñ   ‚ï£‚ï£‚ï£\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚îê  ‚ïô‚ï¨‚ï¨‚ï¨‚ï¨‚îÇ‚ïì‚ï£‚ï£‚ï£‚ïù‚ïú  ‚ï´‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£ ‚ïü‚ï£‚ï£‚ï£‚ïô ‚ïô‚ï£‚ï£‚ï£  ‚ï£‚ï£‚ï£ ‚ïô‚ïü‚ï£‚ï£‚ïú‚ïô  ‚ï´‚ï£‚ï£  ‚ïü‚ï£‚ï£\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚îê     ‚ïô‚ï¨‚ï¨‚ï£‚ï£      ‚ï´‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£ ‚ïü‚ï£‚ï£‚ï¨   ‚ï£‚ï£‚ï£  ‚ï£‚ï£‚ï£  ‚ïü‚ï£‚ï£     ‚ï£‚ï£‚ï£‚îå‚ï£‚ï£‚ïú\n",
            " ‚ï¨‚ï¨‚ï¨‚ïú       ‚ï¨‚ï¨‚ï£‚ï£      ‚ïô‚ïù‚ï£‚ï£‚ï¨      ‚ïô‚ï£‚ï£‚ï£‚ïó‚ïñ‚ïì‚ïó‚ï£‚ï£‚ï£‚ïú ‚ïü‚ï£‚ï£‚ï¨   ‚ï£‚ï£‚ï£  ‚ï£‚ï£‚ï£  ‚ïü‚ï£‚ï£‚ï¶‚ïì    ‚ï£‚ï£‚ï£‚ï£‚ï£\n",
            " ‚ïô   ‚ïì‚ï¶‚ïñ    ‚ï¨‚ï¨‚ï£‚ï£   ‚ïì‚ïó‚ïó‚ïñ            ‚ïô‚ïù‚ï£‚ï£‚ï£‚ï£‚ïù‚ïú   ‚ïò‚ïù‚ïù‚ïú   ‚ïù‚ïù‚ïù  ‚ïù‚ïù‚ïù   ‚ïô‚ï£‚ï£‚ï£    ‚ïü‚ï£‚ï£‚ï£\n",
            "   ‚ï©‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¶‚ï¶‚ï¨‚ï¨‚ï£‚ï£‚ïó‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ïù                                             ‚ï´‚ï£‚ï£‚ï£‚ï£\n",
            "      ‚ïô‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ïù‚ïú\n",
            "          ‚ïô‚ï¨‚ï¨‚ï¨‚ï£‚ï£‚ï£‚ïú\n",
            "             ‚ïô\n",
            "        \n",
            " Version information:\n",
            "  ml-agents: 1.2.0.dev0,\n",
            "  ml-agents-envs: 1.2.0.dev0,\n",
            "  Communicator API: 1.5.0,\n",
            "  PyTorch: 2.6.0+cu124\n",
            "[INFO] Connected to Unity environment with package version 2.1.0-exp.1 and communication version 1.5.0\n",
            "[INFO] Connected new brain: SnowballTarget?team=0\n",
            "[INFO] Hyperparameters for behavior name SnowballTarget: \n",
            "\ttrainer_type:\tppo\n",
            "\thyperparameters:\t\n",
            "\t  batch_size:\t128\n",
            "\t  buffer_size:\t2048\n",
            "\t  learning_rate:\t0.0003\n",
            "\t  beta:\t0.005\n",
            "\t  epsilon:\t0.2\n",
            "\t  lambd:\t0.95\n",
            "\t  num_epoch:\t3\n",
            "\t  shared_critic:\tFalse\n",
            "\t  learning_rate_schedule:\tlinear\n",
            "\t  beta_schedule:\tlinear\n",
            "\t  epsilon_schedule:\tlinear\n",
            "\tcheckpoint_interval:\t50000\n",
            "\tnetwork_settings:\t\n",
            "\t  normalize:\tFalse\n",
            "\t  hidden_units:\t256\n",
            "\t  num_layers:\t2\n",
            "\t  vis_encode_type:\tsimple\n",
            "\t  memory:\tNone\n",
            "\t  goal_conditioning_type:\thyper\n",
            "\t  deterministic:\tFalse\n",
            "\treward_signals:\t\n",
            "\t  extrinsic:\t\n",
            "\t    gamma:\t0.99\n",
            "\t    strength:\t1.0\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t128\n",
            "\t      num_layers:\t2\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\tinit_path:\tNone\n",
            "\tkeep_checkpoints:\t10\n",
            "\teven_checkpoints:\tFalse\n",
            "\tmax_steps:\t200000\n",
            "\ttime_horizon:\t64\n",
            "\tsummary_freq:\t10000\n",
            "\tthreaded:\tFalse\n",
            "\tself_play:\tNone\n",
            "\tbehavioral_cloning:\tNone\n",
            "[INFO] SnowballTarget. Step: 10000. Time Elapsed: 23.979 s. Mean Reward: 3.591. Std of Reward: 2.146. Training.\n",
            "[INFO] SnowballTarget. Step: 20000. Time Elapsed: 45.622 s. Mean Reward: 7.327. Std of Reward: 2.796. Training.\n",
            "[INFO] SnowballTarget. Step: 30000. Time Elapsed: 64.493 s. Mean Reward: 9.591. Std of Reward: 2.339. Training.\n",
            "[INFO] SnowballTarget. Step: 40000. Time Elapsed: 85.536 s. Mean Reward: 10.636. Std of Reward: 2.792. Training.\n",
            "[INFO] SnowballTarget. Step: 50000. Time Elapsed: 105.789 s. Mean Reward: 13.409. Std of Reward: 2.733. Training.\n",
            "[INFO] Exported results/SnowballTarget1/SnowballTarget/SnowballTarget-49936.onnx\n",
            "[INFO] SnowballTarget. Step: 60000. Time Elapsed: 126.147 s. Mean Reward: 15.309. Std of Reward: 2.543. Training.\n",
            "[INFO] SnowballTarget. Step: 70000. Time Elapsed: 147.776 s. Mean Reward: 16.659. Std of Reward: 3.618. Training.\n",
            "[INFO] SnowballTarget. Step: 80000. Time Elapsed: 170.521 s. Mean Reward: 18.673. Std of Reward: 2.886. Training.\n",
            "[INFO] SnowballTarget. Step: 90000. Time Elapsed: 190.919 s. Mean Reward: 21.023. Std of Reward: 2.545. Training.\n",
            "[INFO] SnowballTarget. Step: 100000. Time Elapsed: 212.732 s. Mean Reward: 22.636. Std of Reward: 2.235. Training.\n",
            "[INFO] Exported results/SnowballTarget1/SnowballTarget/SnowballTarget-99960.onnx\n",
            "[INFO] SnowballTarget. Step: 110000. Time Elapsed: 233.656 s. Mean Reward: 23.833. Std of Reward: 2.201. Training.\n",
            "[INFO] SnowballTarget. Step: 120000. Time Elapsed: 255.177 s. Mean Reward: 24.600. Std of Reward: 2.847. Training.\n",
            "[INFO] SnowballTarget. Step: 130000. Time Elapsed: 277.760 s. Mean Reward: 24.745. Std of Reward: 2.306. Training.\n",
            "[INFO] SnowballTarget. Step: 140000. Time Elapsed: 298.455 s. Mean Reward: 24.205. Std of Reward: 2.272. Training.\n",
            "[INFO] SnowballTarget. Step: 150000. Time Elapsed: 320.612 s. Mean Reward: 24.927. Std of Reward: 2.044. Training.\n",
            "[INFO] Exported results/SnowballTarget1/SnowballTarget/SnowballTarget-149984.onnx\n",
            "[INFO] SnowballTarget. Step: 160000. Time Elapsed: 342.036 s. Mean Reward: 25.591. Std of Reward: 2.329. Training.\n",
            "[INFO] SnowballTarget. Step: 170000. Time Elapsed: 363.209 s. Mean Reward: 25.073. Std of Reward: 2.319. Training.\n",
            "[INFO] SnowballTarget. Step: 180000. Time Elapsed: 383.980 s. Mean Reward: 24.682. Std of Reward: 2.652. Training.\n",
            "[INFO] SnowballTarget. Step: 190000. Time Elapsed: 405.896 s. Mean Reward: 25.745. Std of Reward: 2.298. Training.\n",
            "[INFO] SnowballTarget. Step: 200000. Time Elapsed: 425.334 s. Mean Reward: 25.636. Std of Reward: 2.460. Training.\n",
            "[INFO] Exported results/SnowballTarget1/SnowballTarget/SnowballTarget-199984.onnx\n",
            "[INFO] Exported results/SnowballTarget1/SnowballTarget/SnowballTarget-200112.onnx\n",
            "[INFO] Copied results/SnowballTarget1/SnowballTarget/SnowballTarget-200112.onnx to results/SnowballTarget1/SnowballTarget.onnx.\n"
          ]
        }
      ],
      "source": [
        "!mlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id=\"SnowballTarget1\" --no-graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vue94AzPy1t"
      },
      "source": [
        "### Push the agent to the ü§ó Hub\n",
        "\n",
        "- Now that we trained our agent, we‚Äôre **ready to push it to the Hub to be able to visualize it playing on your browserüî•.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izT6FpgNzZ6R"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n",
        "\n",
        "- Copy the token\n",
        "- Run the cell below and paste the token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKt2vsYoK56o"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSU9qD9_6dem"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4fPfnczunT"
      },
      "source": [
        "Then, we simply need to run `mlagents-push-to-hf`.\n",
        "\n",
        "And we define 4 parameters:\n",
        "\n",
        "1. `--run-id`: the name of the training run id.\n",
        "2. `--local-dir`: where the agent was saved, it‚Äôs results/<run_id name>, so in my case results/First Training.\n",
        "3. `--repo-id`: the name of the Hugging Face repo you want to create or update. It‚Äôs always <your huggingface username>/<the repo name>\n",
        "If the repo does not exist **it will be created automatically**\n",
        "4. `--commit-message`: since HF repos are git repository you need to define a commit message.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentspushtohub.png\" alt=\"Push to Hub\"/>\n",
        "\n",
        "For instance:\n",
        "\n",
        "`!mlagents-push-to-hf  --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\"  --commit-message=\"First Push\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kAFzVB7OYj_H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] This function will create a model card and upload your SnowballTarget1 into HuggingFace Hub. This is a work in progress: If you encounter a bug, please send open an issue\n",
            "[INFO] Pushing repo SnowballTarget1 to the Hugging Face Hub\n",
            "SnowballTarget.onnx:   0%|                           | 0.00/651k [00:00<?, ?B/s]\n",
            "SnowballTarget-149984.onnx:   0%|                    | 0.00/651k [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "SnowballTarget-149984.pt:   0%|                     | 0.00/3.85M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "SnowballTarget-199984.pt:   0%|                     | 0.00/3.85M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 13 LFS files:   0%|                               | 0/13 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SnowballTarget.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 651k/651k [00:00<00:00, 3.04MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "SnowballTarget-149984.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.85M/3.85M [00:00<00:00, 11.4MB/s]\n",
            "SnowballTarget-199984.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.85M/3.85M [00:00<00:00, 9.67MB/s]\n",
            "SnowballTarget.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 651k/651k [00:00<00:00, 1.56MB/s]\n",
            "SnowballTarget-149984.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 651k/651k [00:00<00:00, 1.47MB/s]\n",
            "\n",
            "SnowballTarget-200112.pt:  45%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé      | 1.72M/3.85M [00:00<00:00, 12.5MB/s]\u001b[A\n",
            "\n",
            "SnowballTarget-49936.pt:   0%|                      | 0.00/3.85M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "SnowballTarget-99960.onnx:   0%|                     | 0.00/651k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 13 LFS files:   8%|‚ñà‚ñä                     | 1/13 [00:00<00:06,  1.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SnowballTarget-200112.pt:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 2.98M/3.85M [00:00<00:00, 10.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "SnowballTarget-49936.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 651k/651k [00:00<00:00, 3.15MB/s]\u001b[A\u001b[A\n",
            "\n",
            "checkpoint.pt:   0%|                                | 0.00/3.85M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "SnowballTarget-99960.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.85M/3.85M [00:00<00:00, 10.7MB/s]\u001b[A\u001b[A\n",
            "SnowballTarget-99960.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 651k/651k [00:00<00:00, 1.54MB/s]\n",
            "SnowballTarget-200112.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.85M/3.85M [00:00<00:00, 5.33MB/s]\n",
            "SnowballTarget-49936.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.85M/3.85M [00:00<00:00, 6.13MB/s]\n",
            "\n",
            "\n",
            "checkpoint.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.85M/3.85M [00:00<00:00, 9.54MB/s]\u001b[A\u001b[A\n",
            "events.out.tfevents.1743189358.soal-10.stanford.edu.3870786.0: 100%|‚ñà| 23.5k/23.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Upload 13 LFS files:  46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå            | 6/13 [00:01<00:01,  5.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "SnowballTarget.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 651k/651k [00:00<00:00, 1.69MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Upload 13 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:01<00:00,  8.10it/s]\n",
            "[INFO] Your model is pushed to the hub. You can view your model here: https://huggingface.co/stalaei/DeepRL-ppo-SnowballTarget\n"
          ]
        }
      ],
      "source": [
        "!mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"stalaei/DeepRL-ppo-SnowballTarget\" --commit-message=\"First Push\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGEFAIboLVc6"
      },
      "outputs": [],
      "source": [
        "!mlagents-push-to-hf  --run-id= # Add your run id  --local-dir= # Your local dir  --repo-id= # Your repo id  --commit-message= # Your commit message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yborB0850FTM"
      },
      "source": [
        "Else, if everything worked you should have this at the end of the process(but with a different url üòÜ) :\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Your model is pushed to the hub. You can view your model here: https://huggingface.co/ThomasSimonini/ppo-SnowballTarget\n",
        "```\n",
        "\n",
        "It‚Äôs the link to your model, it contains a model card that explains how to use it, your Tensorboard and your config file. **What‚Äôs awesome is that it‚Äôs a git repository, that means you can have different commits, update your repository with a new push etc.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uaon2cg0NrL"
      },
      "source": [
        "But now comes the best: **being able to visualize your agent online üëÄ.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMc4oOsE0QiZ"
      },
      "source": [
        "### Watch your agent playing üëÄ\n",
        "\n",
        "For this step it‚Äôs simple:\n",
        "\n",
        "1. Go here: https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget\n",
        "\n",
        "2. Launch the game and put it in full screen by clicking on the bottom right button\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_load.png\" alt=\"Snowballtarget load\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djs8c5rR0Z8a"
      },
      "source": [
        "1. In step 1, type your username (your username is case sensitive: for instance, my username is ThomasSimonini not thomassimonini or ThOmasImoNInI) and click on the search button.\n",
        "\n",
        "2. In step 2, select your model repository.\n",
        "\n",
        "3. In step 3, **choose which model you want to replay**:\n",
        "  - I have multiple ones, since we saved a model every 500000 timesteps.\n",
        "  - But since I want the more recent, I choose `SnowballTarget.onnx`\n",
        "\n",
        "üëâ What‚Äôs nice **is to try with different models step to see the improvement of the agent.**\n",
        "\n",
        "And don't hesitate to share the best score your agent gets on discord in #rl-i-made-this channel üî•\n",
        "\n",
        "Let's now try a harder environment called Pyramids..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVMwRi4y_tmx"
      },
      "source": [
        "## Pyramids üèÜ\n",
        "\n",
        "### Download and move the environment zip file in `./training-envs-executables/linux/`\n",
        "- Our environment executable is in a zip file.\n",
        "- We need to download it and place it to `./training-envs-executables/linux/`\n",
        "- We use a linux executable because we use colab, and colab machines OS is Ubuntu (linux)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2C48SGZjZYw"
      },
      "source": [
        "We downloaded the file Pyramids.zip from from https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip using `wget`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eWh8Pl3sjZY2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-28 13:28:45--  https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip\n",
            "Resolving huggingface.co (huggingface.co)... 108.138.246.71, 108.138.246.67, 108.138.246.85, ...\n",
            "Connecting to huggingface.co (huggingface.co)|108.138.246.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/f2/c7/f2c7eed6c9ed94477803abde6483db87a56a1f82597e00780698db5998afdcda/1e0f1cfa88a380b42644d974525a7dbfe144089883a401526b8341c5441f7cae?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Pyramids.zip%3B+filename%3D%22Pyramids.zip%22%3B&response-content-type=application%2Fzip&Expires=1743197325&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzE5NzMyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mMi9jNy9mMmM3ZWVkNmM5ZWQ5NDQ3NzgwM2FiZGU2NDgzZGI4N2E1NmExZjgyNTk3ZTAwNzgwNjk4ZGI1OTk4YWZkY2RhLzFlMGYxY2ZhODhhMzgwYjQyNjQ0ZDk3NDUyNWE3ZGJmZTE0NDA4OTg4M2E0MDE1MjZiODM0MWM1NDQxZjdjYWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=o%7E4l1RtanykNKMKgNz9NffRrqblLUW-AvI3WOQ2fPnKc5yknJdof6Q2Zj7Q9MOY8M%7Er%7En98xl6bkQnfbmYIUxjFjoaPErUfcQgk4pT22dr1CCh3iugrv0klyQZChKJddahbfXNpFbIjq%7E6IlIaT15Yhw4ykivFAOgOI%7EkE%7E1yfrZJ2ke6g8ytxEmzuY8fXafpAvds4aULTCvQ-3UQeQHHTTVbEmv4pER1anZz9oK9lsa%7E64RgjC-6tfeItIp0%7ED%7EBHtg4ROoIwwA6bZFrpwbwoJsnxeClg3wWCqzN8mfTr8JVT8bAZ4Ky4loZktYVZSnRGO3yqshI-40nFdFNU6KwA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-03-28 13:28:45--  https://cdn-lfs.hf.co/repos/f2/c7/f2c7eed6c9ed94477803abde6483db87a56a1f82597e00780698db5998afdcda/1e0f1cfa88a380b42644d974525a7dbfe144089883a401526b8341c5441f7cae?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Pyramids.zip%3B+filename%3D%22Pyramids.zip%22%3B&response-content-type=application%2Fzip&Expires=1743197325&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzE5NzMyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mMi9jNy9mMmM3ZWVkNmM5ZWQ5NDQ3NzgwM2FiZGU2NDgzZGI4N2E1NmExZjgyNTk3ZTAwNzgwNjk4ZGI1OTk4YWZkY2RhLzFlMGYxY2ZhODhhMzgwYjQyNjQ0ZDk3NDUyNWE3ZGJmZTE0NDA4OTg4M2E0MDE1MjZiODM0MWM1NDQxZjdjYWU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=o%7E4l1RtanykNKMKgNz9NffRrqblLUW-AvI3WOQ2fPnKc5yknJdof6Q2Zj7Q9MOY8M%7Er%7En98xl6bkQnfbmYIUxjFjoaPErUfcQgk4pT22dr1CCh3iugrv0klyQZChKJddahbfXNpFbIjq%7E6IlIaT15Yhw4ykivFAOgOI%7EkE%7E1yfrZJ2ke6g8ytxEmzuY8fXafpAvds4aULTCvQ-3UQeQHHTTVbEmv4pER1anZz9oK9lsa%7E64RgjC-6tfeItIp0%7ED%7EBHtg4ROoIwwA6bZFrpwbwoJsnxeClg3wWCqzN8mfTr8JVT8bAZ4Ky4loZktYVZSnRGO3yqshI-40nFdFNU6KwA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.155.202.123, 18.155.202.10, 18.155.202.83, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.155.202.123|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42907187 (41M) [application/zip]\n",
            "Saving to: ‚Äò./training-envs-executables/linux/Pyramids.zip‚Äô\n",
            "\n",
            "./training-envs-exe 100%[===================>]  40.92M   174MB/s    in 0.2s    \n",
            "\n",
            "2025-03-28 13:28:46 (174 MB/s) - ‚Äò./training-envs-executables/linux/Pyramids.zip‚Äô saved [42907187/42907187]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip\" -O ./training-envs-executables/linux/Pyramids.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5LXPOPujZY3"
      },
      "source": [
        "We unzip the executable.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SmNgFdXhjZY3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/Pyramids.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1jxwhrJjZY3"
      },
      "source": [
        "Make sure your file is accessible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6fDd03btjZY3"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./training-envs-executables/linux/Pyramids/Pyramids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqceIATXAgih"
      },
      "source": [
        "###  Modify the PyramidsRND config file\n",
        "- Contrary to the first environment which was a custom one, **Pyramids was made by the Unity team**.\n",
        "- So the PyramidsRND config file already exists and is in ./content/ml-agents/config/ppo/PyramidsRND.yaml\n",
        "- You might asked why \"RND\" in PyramidsRND. RND stands for *random network distillation* it's a way to generate curiosity rewards. If you want to know more on that we wrote an article explaning this technique: https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938\n",
        "\n",
        "For this training, we‚Äôll modify one thing:\n",
        "- The total training steps hyperparameter is too high since we can hit the benchmark (mean reward = 1.75) in only 1M training steps.\n",
        "üëâ To do that, we go to config/ppo/PyramidsRND.yaml,**and modify these to max_steps to 1000000.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-config.png\" alt=\"Pyramids config\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI-5aPL7BWVk"
      },
      "source": [
        "As an experimentation, you should also try to modify some other hyperparameters, Unity provides a very [good documentation explaining each of them here](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
        "\n",
        "We‚Äôre now ready to train our agent üî•."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5hr1rvIBdZH"
      },
      "source": [
        "### Train the agent\n",
        "\n",
        "The training will take 30 to 45min depending on your machine, go take a ‚òïÔ∏èyou deserve it ü§ó."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fXi4-IaHBhqD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "            ‚îê  ‚ïñ\n",
            "        ‚ïì‚ïñ‚ï¨‚îÇ‚ï°  ‚îÇ‚îÇ‚ï¨‚ïñ‚ïñ\n",
            "    ‚ïì‚ïñ‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚îò  ‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚ï¨‚ïñ\n",
            " ‚ïñ‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚ï¨‚ïú        ‚ïô‚ï¨‚îÇ‚îÇ‚îÇ‚îÇ‚îÇ‚ïñ‚ïñ                               ‚ïó‚ïó‚ïó\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚ïñ‚îÇ‚îÇ‚ï¶‚ïñ        ‚ïñ‚ï¨‚îÇ‚îÇ‚ïó‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£             ‚ïú‚ïú‚ïú  ‚ïü‚ï£‚ï£\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ïñ‚îÇ‚ï¨‚ïñ‚ïñ‚ïì‚ï¨‚ï™‚îÇ‚ïì‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£ ‚ïí‚ï£‚ï£‚ïñ‚ïó‚ï£‚ï£‚ï£‚ïó   ‚ï£‚ï£‚ï£ ‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£ ‚ïü‚ï£‚ï£‚ïñ   ‚ï£‚ï£‚ï£\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚îê  ‚ïô‚ï¨‚ï¨‚ï¨‚ï¨‚îÇ‚ïì‚ï£‚ï£‚ï£‚ïù‚ïú  ‚ï´‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£ ‚ïü‚ï£‚ï£‚ï£‚ïô ‚ïô‚ï£‚ï£‚ï£  ‚ï£‚ï£‚ï£ ‚ïô‚ïü‚ï£‚ï£‚ïú‚ïô  ‚ï´‚ï£‚ï£  ‚ïü‚ï£‚ï£\n",
            " ‚ï¨‚ï¨‚ï¨‚ï¨‚îê     ‚ïô‚ï¨‚ï¨‚ï£‚ï£      ‚ï´‚ï£‚ï£‚ï£‚ï¨      ‚ïü‚ï£‚ï£‚ï¨    ‚ïü‚ï£‚ï£‚ï£ ‚ïü‚ï£‚ï£‚ï¨   ‚ï£‚ï£‚ï£  ‚ï£‚ï£‚ï£  ‚ïü‚ï£‚ï£     ‚ï£‚ï£‚ï£‚îå‚ï£‚ï£‚ïú\n",
            " ‚ï¨‚ï¨‚ï¨‚ïú       ‚ï¨‚ï¨‚ï£‚ï£      ‚ïô‚ïù‚ï£‚ï£‚ï¨      ‚ïô‚ï£‚ï£‚ï£‚ïó‚ïñ‚ïì‚ïó‚ï£‚ï£‚ï£‚ïú ‚ïü‚ï£‚ï£‚ï¨   ‚ï£‚ï£‚ï£  ‚ï£‚ï£‚ï£  ‚ïü‚ï£‚ï£‚ï¶‚ïì    ‚ï£‚ï£‚ï£‚ï£‚ï£\n",
            " ‚ïô   ‚ïì‚ï¶‚ïñ    ‚ï¨‚ï¨‚ï£‚ï£   ‚ïì‚ïó‚ïó‚ïñ            ‚ïô‚ïù‚ï£‚ï£‚ï£‚ï£‚ïù‚ïú   ‚ïò‚ïù‚ïù‚ïú   ‚ïù‚ïù‚ïù  ‚ïù‚ïù‚ïù   ‚ïô‚ï£‚ï£‚ï£    ‚ïü‚ï£‚ï£‚ï£\n",
            "   ‚ï©‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¶‚ï¶‚ï¨‚ï¨‚ï£‚ï£‚ïó‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ïù                                             ‚ï´‚ï£‚ï£‚ï£‚ï£\n",
            "      ‚ïô‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï¨‚ï£‚ï£‚ï£‚ï£‚ï£‚ï£‚ïù‚ïú\n",
            "          ‚ïô‚ï¨‚ï¨‚ï¨‚ï£‚ï£‚ï£‚ïú\n",
            "             ‚ïô\n",
            "        \n",
            " Version information:\n",
            "  ml-agents: 1.2.0.dev0,\n",
            "  ml-agents-envs: 1.2.0.dev0,\n",
            "  Communicator API: 1.5.0,\n",
            "  PyTorch: 2.6.0+cu124\n",
            "[INFO] Connected to Unity environment with package version 2.2.1-exp.1 and communication version 1.5.0\n",
            "[INFO] Connected new brain: Pyramids?team=0\n",
            "[INFO] Hyperparameters for behavior name Pyramids: \n",
            "\ttrainer_type:\tppo\n",
            "\thyperparameters:\t\n",
            "\t  batch_size:\t128\n",
            "\t  buffer_size:\t2048\n",
            "\t  learning_rate:\t0.0003\n",
            "\t  beta:\t0.01\n",
            "\t  epsilon:\t0.2\n",
            "\t  lambd:\t0.95\n",
            "\t  num_epoch:\t3\n",
            "\t  shared_critic:\tFalse\n",
            "\t  learning_rate_schedule:\tlinear\n",
            "\t  beta_schedule:\tlinear\n",
            "\t  epsilon_schedule:\tlinear\n",
            "\tcheckpoint_interval:\t500000\n",
            "\tnetwork_settings:\t\n",
            "\t  normalize:\tFalse\n",
            "\t  hidden_units:\t512\n",
            "\t  num_layers:\t2\n",
            "\t  vis_encode_type:\tsimple\n",
            "\t  memory:\tNone\n",
            "\t  goal_conditioning_type:\thyper\n",
            "\t  deterministic:\tFalse\n",
            "\treward_signals:\t\n",
            "\t  extrinsic:\t\n",
            "\t    gamma:\t0.99\n",
            "\t    strength:\t1.0\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t128\n",
            "\t      num_layers:\t2\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\t  rnd:\t\n",
            "\t    gamma:\t0.99\n",
            "\t    strength:\t0.01\n",
            "\t    network_settings:\t\n",
            "\t      normalize:\tFalse\n",
            "\t      hidden_units:\t64\n",
            "\t      num_layers:\t3\n",
            "\t      vis_encode_type:\tsimple\n",
            "\t      memory:\tNone\n",
            "\t      goal_conditioning_type:\thyper\n",
            "\t      deterministic:\tFalse\n",
            "\t    learning_rate:\t0.0001\n",
            "\t    encoding_size:\tNone\n",
            "\tinit_path:\tNone\n",
            "\tkeep_checkpoints:\t5\n",
            "\teven_checkpoints:\tFalse\n",
            "\tmax_steps:\t1000000\n",
            "\ttime_horizon:\t128\n",
            "\tsummary_freq:\t30000\n",
            "\tthreaded:\tFalse\n",
            "\tself_play:\tNone\n",
            "\tbehavioral_cloning:\tNone\n",
            "[INFO] Pyramids. Step: 30000. Time Elapsed: 47.240 s. Mean Reward: -1.000. Std of Reward: 0.000. Training.\n",
            "[INFO] Pyramids. Step: 60000. Time Elapsed: 91.620 s. Mean Reward: -0.931. Std of Reward: 0.393. Training.\n",
            "[INFO] Pyramids. Step: 90000. Time Elapsed: 138.809 s. Mean Reward: -0.932. Std of Reward: 0.375. Training.\n",
            "[INFO] Pyramids. Step: 120000. Time Elapsed: 184.567 s. Mean Reward: -0.925. Std of Reward: 0.418. Training.\n",
            "[INFO] Pyramids. Step: 150000. Time Elapsed: 230.031 s. Mean Reward: -0.855. Std of Reward: 0.570. Training.\n",
            "[INFO] Pyramids. Step: 180000. Time Elapsed: 273.618 s. Mean Reward: -0.583. Std of Reward: 0.889. Training.\n",
            "[INFO] Pyramids. Step: 210000. Time Elapsed: 316.568 s. Mean Reward: -0.559. Std of Reward: 0.959. Training.\n",
            "[INFO] Pyramids. Step: 240000. Time Elapsed: 362.317 s. Mean Reward: -0.537. Std of Reward: 0.947. Training.\n",
            "[INFO] Pyramids. Step: 270000. Time Elapsed: 409.130 s. Mean Reward: -0.183. Std of Reward: 1.109. Training.\n",
            "[INFO] Pyramids. Step: 300000. Time Elapsed: 455.531 s. Mean Reward: -0.237. Std of Reward: 1.085. Training.\n",
            "[INFO] Pyramids. Step: 330000. Time Elapsed: 501.277 s. Mean Reward: 0.423. Std of Reward: 1.234. Training.\n",
            "[INFO] Pyramids. Step: 360000. Time Elapsed: 549.378 s. Mean Reward: 0.165. Std of Reward: 1.206. Training.\n",
            "[INFO] Pyramids. Step: 390000. Time Elapsed: 597.696 s. Mean Reward: 0.792. Std of Reward: 1.145. Training.\n",
            "[INFO] Pyramids. Step: 420000. Time Elapsed: 644.359 s. Mean Reward: 0.385. Std of Reward: 1.192. Training.\n",
            "[INFO] Pyramids. Step: 450000. Time Elapsed: 692.113 s. Mean Reward: 0.616. Std of Reward: 1.166. Training.\n",
            "[INFO] Pyramids. Step: 480000. Time Elapsed: 738.206 s. Mean Reward: 0.709. Std of Reward: 1.165. Training.\n",
            "[INFO] Exported results/Pyramids Training/Pyramids/Pyramids-499875.onnx\n",
            "[INFO] Pyramids. Step: 510000. Time Elapsed: 784.696 s. Mean Reward: 0.341. Std of Reward: 1.288. Training.\n",
            "[INFO] Pyramids. Step: 540000. Time Elapsed: 832.102 s. Mean Reward: 0.948. Std of Reward: 1.046. Training.\n",
            "[INFO] Pyramids. Step: 570000. Time Elapsed: 877.767 s. Mean Reward: 0.879. Std of Reward: 1.094. Training.\n",
            "[INFO] Pyramids. Step: 600000. Time Elapsed: 924.093 s. Mean Reward: 1.133. Std of Reward: 0.917. Training.\n",
            "[INFO] Pyramids. Step: 630000. Time Elapsed: 971.561 s. Mean Reward: 1.238. Std of Reward: 0.906. Training.\n",
            "[INFO] Pyramids. Step: 660000. Time Elapsed: 1021.405 s. Mean Reward: 1.141. Std of Reward: 0.982. Training.\n",
            "[INFO] Pyramids. Step: 690000. Time Elapsed: 1069.236 s. Mean Reward: 1.239. Std of Reward: 0.841. Training.\n",
            "[INFO] Pyramids. Step: 720000. Time Elapsed: 1118.508 s. Mean Reward: 1.371. Std of Reward: 0.762. Training.\n",
            "[INFO] Pyramids. Step: 750000. Time Elapsed: 1163.629 s. Mean Reward: 1.334. Std of Reward: 0.840. Training.\n",
            "[INFO] Pyramids. Step: 780000. Time Elapsed: 1213.039 s. Mean Reward: 1.393. Std of Reward: 0.741. Training.\n",
            "[INFO] Pyramids. Step: 810000. Time Elapsed: 1259.212 s. Mean Reward: 1.300. Std of Reward: 0.828. Training.\n",
            "[INFO] Pyramids. Step: 840000. Time Elapsed: 1310.093 s. Mean Reward: 1.440. Std of Reward: 0.719. Training.\n",
            "[INFO] Pyramids. Step: 870000. Time Elapsed: 1360.074 s. Mean Reward: 1.377. Std of Reward: 0.762. Training.\n",
            "[INFO] Pyramids. Step: 900000. Time Elapsed: 1412.393 s. Mean Reward: 1.193. Std of Reward: 0.963. Training.\n",
            "[INFO] Pyramids. Step: 930000. Time Elapsed: 1462.708 s. Mean Reward: 1.493. Std of Reward: 0.636. Training.\n",
            "[INFO] Pyramids. Step: 960000. Time Elapsed: 1513.471 s. Mean Reward: 1.464. Std of Reward: 0.567. Training.\n",
            "[INFO] Pyramids. Step: 990000. Time Elapsed: 1562.183 s. Mean Reward: 1.455. Std of Reward: 0.504. Training.\n",
            "[INFO] Exported results/Pyramids Training/Pyramids/Pyramids-999933.onnx\n",
            "[INFO] Exported results/Pyramids Training/Pyramids/Pyramids-1000061.onnx\n",
            "[INFO] Copied results/Pyramids Training/Pyramids/Pyramids-1000061.onnx to results/Pyramids Training/Pyramids.onnx.\n"
          ]
        }
      ],
      "source": [
        "!mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./training-envs-executables/linux/Pyramids/Pyramids --run-id=\"Pyramids Training\" --no-graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txonKxuSByut"
      },
      "source": [
        "### Push the agent to the ü§ó Hub\n",
        "\n",
        "- Now that we trained our agent, we‚Äôre **ready to push it to the Hub to be able to visualize it playing on your browserüî•.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yiEQbv7rB4mU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] This function will create a model card and upload your Pyramids Training into HuggingFace Hub. This is a work in progress: If you encounter a bug, please send open an issue\n",
            "[INFO] Pushing repo Pyramids Training to the Hugging Face Hub\n",
            "Upload 9 LFS files:   0%|                                 | 0/9 [00:00<?, ?it/s]\n",
            "Pyramids.onnx:   0%|                                | 0.00/1.42M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Pyramids.onnx:   0%|                                | 0.00/1.42M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Pyramids-499875.pt:   0%|                           | 0.00/8.66M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Pyramids-1000061.pt:   0%|                          | 0.00/8.66M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pyramids-499875.onnx:   0%|                         | 0.00/1.42M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Pyramids-1000061.pt:  24%|‚ñà‚ñà‚ñà‚ñà             | 2.08M/8.66M [00:00<00:00, 20.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pyramids-499875.onnx:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä          | 573k/1.42M [00:00<00:00, 4.75MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Pyramids-499875.pt:  18%|‚ñà‚ñà‚ñà‚ñé              | 1.57M/8.66M [00:00<00:00, 10.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Pyramids-1000061.pt:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä        | 4.49M/8.66M [00:00<00:00, 21.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Pyramids.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42M/1.42M [00:00<00:00, 4.28MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 9 LFS files:  11%|‚ñà‚ñà‚ñä                      | 1/9 [00:00<00:04,  1.94it/s]\u001b[A\u001b[A\u001b[A\n",
            "Pyramids-1000061.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.66M/8.66M [00:00<00:00, 15.2MB/s]\u001b[A\n",
            "Pyramids.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42M/1.42M [00:00<00:00, 2.35MB/s]\n",
            "\n",
            "Pyramids-499875.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.66M/8.66M [00:00<00:00, 12.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "Pyramids-499875.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42M/1.42M [00:00<00:00, 1.85MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "events.out.tfevents.1743193813.soal-10.stanford.edu.4030964.0:   0%| | 0.00/333k\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "checkpoint.pt:  30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                | 2.56M/8.66M [00:00<00:00, 15.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "events.out.tfevents.1743193813.soal-10.stanford.edu.4030964.0: 100%|‚ñà| 333k/333k\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Pyramids-999933.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.66M/8.66M [00:00<00:00, 13.0MB/s]\u001b[A\u001b[A\n",
            "checkpoint.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.66M/8.66M [00:00<00:00, 12.5MB/s]\n",
            "Pyramids.onnx: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42M/1.42M [00:00<00:00, 2.94MB/s]\n",
            "Upload 9 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:01<00:00,  5.98it/s]\n",
            "[INFO] Your model is pushed to the hub. You can view your model here: https://huggingface.co/stalaei/DeepRL-ppo-Pyramids\n"
          ]
        }
      ],
      "source": [
        "# !mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"stalaei/DeepRL-ppo-SnowballTarget\" --commit-message=\"First Push\"\n",
        "!mlagents-push-to-hf  --run-id=\"Pyramids Training\" --local-dir=\"./results/Pyramids Training\" --repo-id=\"stalaei/DeepRL-ppo-Pyramids\" --commit-message=\"First Push\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aZfgxo-CDeQ"
      },
      "source": [
        "### Watch your agent playing üëÄ\n",
        "\n",
        "üëâ https://huggingface.co/spaces/unity/ML-Agents-Pyramids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGG_oq2n0wjB"
      },
      "source": [
        "### üéÅ Bonus: Why not train on another environment?\n",
        "Now that you know how to train an agent using MLAgents, **why not try another environment?**\n",
        "\n",
        "MLAgents provides 17 different and we‚Äôre building some custom ones. The best way to learn is to try things of your own, have fun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSAkJxSr0z6-"
      },
      "source": [
        "![cover](https://miro.medium.com/max/1400/0*xERdThTRRM2k_U9f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiyF4FX-04JB"
      },
      "source": [
        "You have the full list of the Unity official environments here üëâ https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md\n",
        "\n",
        "For the demos to visualize your agent üëâ https://huggingface.co/unity\n",
        "\n",
        "For now we have integrated:\n",
        "- [Worm](https://huggingface.co/spaces/unity/ML-Agents-Worm) demo where you teach a **worm to crawl**.\n",
        "- [Walker](https://huggingface.co/spaces/unity/ML-Agents-Walker) demo where you teach an agent **to walk towards a goal**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6dPWmh064H"
      },
      "source": [
        "That‚Äôs all for today. Congrats on finishing this tutorial!\n",
        "\n",
        "The best way to learn is to practice and try stuff. Why not try another environment? ML-Agents has 17 different environments, but you can also create your own? Check the documentation and have fun!\n",
        "\n",
        "See you on Unit 6 üî•,\n",
        "\n",
        "## Keep Learning, Stay  awesome ü§ó"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "DeepRL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
